---
title: ""
date: 
draft: true
---

SCIENCE
a_cycling_theory_of_scientific_process

<p>
In my wanderings of quantitative biology as a means of scientific 
inquiry, I've developed a model for how science can be done - 
or rather understood to be done after the fact.
</p>

<p>
I've translated this into advice, or directives as one sees fit,
below.
Where I can, I've desperatly tried to invoke arcane sounding 
vocabulary to lend a limited sense of authenticity, but do not let 
this mislead you from understanding this as a very modern perspective
from exposure to genomics-ish work from 2010 to 2016.
</p>

<p>
( I include an example for how this would be done to investigate the
dynamics of one particular transcript during a stereotypical 
environmental shift, using qPCR. )
</p>

<hr>

<ul>
  <li><b>First, the practitioner should construct the apparatus</b>.
  In this step, read what has been done and learn the theory of what
  you expect to happen. From this, we can develop the apparatus, or
  the tools and methodology requisite to investigate the phenomenon
  in question in a cleanly interpretable way.

  In our example, this would be the design and verification of 
  primers for qPCR, the verificaiton that culturing methodology
  and sampling are correct, that normalization and other assumptions
  seem reasonable.

  Here is a place where others can really help kick this along in
  progress, but also a place where prior work and help from others
  can bottleneck your exploration down to what is limited by prior
  assumptions. Take advantage of other's help, but dare to be 
  independent - within reason. If you're too close or too far, you
  may not see the phenomenon that explains a lot.
  </li>
  <li>
  <b>Second, explore and wander the wilderness</b>.
  By this, I mean to value breadth of exploring first before 
  hypothesis testing. Here, we fish for hypotheses to actually test.
  
  High-throughput (many samples) and broad-throughput (many variables)
  approaches are valuable here.
  Deciding the scale of what to investigate is critical, and should be
  flexible to changing. By scale, I mean the scope of investigation
  should be varied to broaden the investigation, to make sure things
  behave broadly as we expect them to, and then to narrow the use of
  limited resources to a focused point of veracity.
  More timepoints would be a reasonable focus over replicates, in
  this stage.

  In our example, I would imagine measuring our transcript of
  interest across many timepoints without replicates. We would aim
  to sketch out the interesting periods. Too infrequent of measures
  and we lose structure, too frequent and we lose time and money.
  Balance, but wander.
  </li>
  <li>
  <b>Third, formulate a model</b>.
  If you wandered well, you should have a vauge idea of the underlying
  simplified structure of the phenomenon, what causes it and how to
  think about it in a useful and compacted way. 

  If you don't, then collect all your observations, especially
  those that you have a vague intuition about, and meditate on how
  they could all fit together. 
  If this is not apparent, wander more.

  Loneliness really hurts the researcher in this, and in the next
  stage.

  In our example: 
  perhaps you observe that the cells do a particular thing, and 
  although unrelated to your phenotype of interest is similar to the
  activation of a pathway in a different kind of shift. And maybe you
  wonder if this particular signalling pathway is active during your
  transition and may be involved in your phenotype. 
  Perhaps you're interested in it's involvement in a particular 
  change in rates of some dynamics.
  </li>
  <li>
  <b>Fourth, test the model</b>.
  This is the easiest part, and is the one most folks think of
  as the scientific process.

  Having done all the hard work of proposing a model, or more usefully
  a series of models, you should be able to propose a way to make a
  pertubation to some element of the model. This must in some way
  generate a measureable output that would be consistent or 
  inconsistent with your model.
  Then, perturb the system and see if the output matches your 
  prediction. The methodology of the pertubation may require a return
  to step one.

  Think of this as an excercise of usefulness of the model, ie is
  this condensed representation of what you think of going on useful?
  Personally, I'm agnostic to the imeplementation of the model so
  long as it is useful. Voodoo may be phenomenological, but if it
  has predictive power than the model must be evaluated on that 
  criteria.
  What is its explanatory power?

  Having disrupted some pathway of interest, do the dynamics change
  as expected?
  </li>
  <li>
  <b>Fifth, compress and communicate the model</b>.
  This is an essential skill that has eluded me for quite some time,
  or rather I eluded it through procrastination and foolish
  prioritization.

  Write and talk to people about your model and the evidence for it.
  You must get feedback from your peers and comrades in the 
  investigatory field.

  In our example, presenting the work necessitates the investigator
  determines that the pathway is or is not involved in the changes in
  dynamics of the transcript, or the like. Here is where other 
  investigators may share unpublished observations, other 
  interpretations, or other supporting or critical commentary.
  This feedback is precious and scarce.
  </li>
</ul>

<hr>

<p>
Reconciling models that have various extents of explanatory power
will likely require additional testing. Gaps that cannot be filled
will need more wandering and thinking. Eventually, many models can
be simplified together.
</p>

<p>
Each simplification that keeps its predictive value is a powerful
focusing that does not detract from what is learned but rather spreads 
the distillation of it around the world.
</p>

<p>
Hyperbole, hyperbole, I hope that was useful.
</p>

---
title: ""
date: 
draft: true
---

# Authoritative Science

Fredrick J Ross' wrote a nice book called "Into the Sciences".
One of the most compelling lessons articulated was that of the
transition from student to a practitioner of the scientific process.
In the book, this is described as going from a mode of learning facts
to learning how to question, verify, and make sense of new 
information. He also nicely brought up the case of how science history
tends to be a story of Greats without much of the details of how
that person got to that point with those tools and those people
in that moment. In particular, the underlings and predecessors who
drive a lot of science tend to be forgotten (unless it becomes
advantageous to 
[invoke their plight](http://www.sciencedirect.com/science/article/pii/S0092867415017055)
).

This interpretation makes a lot of sense to me, that one of the major
challenges to new practitioners is transitioning from learning to
teaching. Some folks aren't well suited for both, and while linked
I think that one is not neccesarily a good predictor of the other.

It started to make even more sense when the 2016 US presidential
primary election started to develop, in particular reading about the 
[rise of American authoritarianism](http://www.vox.com/2016/3/1/11127424/trump-authoritarianism)
in both the field of political science and over the preceeding
50 years of domestic political wrangling.

How so? My theory is that if an academic, 
say an undergrad or grad student,
is challenged and undermined in their confidence, then that academic
may cling to orthodoxy in an act of self-preservation. 
Conversly, only those who are encouraged and emboldened, 
by a personal sense of security but potentially by systemic biases
towards certain groups, are thus able to challenge the accepted
wisdom of the Greats and by doing so either strenghten or reform
this canon.

---
title: "Contraints canalyze to enable"
date: 2017-11-12
draft: true
---


http://newt.phys.unsw.edu.au/~jw/sailing.html

javascript tumble bacteria chemotaxis canalization as tumble frequency
---
title: ""
date: 
draft: true
---
SCIENCE
discretion_is_the_easy_part_of_counting?

<p>
intro
The motivation for this thinking comes from thinking about timing in
sample collection. 
</p>

<p>
Often people will collect and present timeseries
with replicates taken and precisely the same times: 0, 5, and 10min.
This, I assume, denotes the time that a sample was taken and a
somewhat reproducible fixation procedure takes place.
When I started to do this, I chose that instead of recording the time
of sample collection to instead record the time of fixation.
While this seems like a small thing, the omission of this data from
prior lab member's work had hidden a very interesting confounding
bit of biology from our analysis, namely the last round of 
transcription before txt synthesis shuts off.
</p>

<p>
So I'm advocating that instead of sampling the discretely-same thing 
over and over, say timepoint 5min exactly time and time again, to
instead sample over continuous time and record exactly when that
happened, with some error of course. Discrepancy between sampling
times should not be blurred and confounded into assumptions of
discretely-identical sampling times, but should be modeled.
</p>

<p>
My thinking about this is well clarified in reading <a href="http://www.biostathandbook.com/variabletypes.html">book on stats</a>.
People treat time as a nominal variable that is set and controlled
by the experimenter. Relinquishing this control in favor of treating
time as a thing to be measured is more honest.
</p>

<p>
Now I'm trying to think of how this all fits into transposon 
mutagenesis.
</p>

